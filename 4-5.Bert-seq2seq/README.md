# Bert for seq2seq

## 1. Reference

- https://github.com/920232796/bert_seq2seq
- [从语言模型到Seq2Seq：Transformer如戏，全靠Mask](https://kexue.fm/archives/6933)

## 2. 示例
```
token_ids: 
[[1,5,6,7,8,3,9,6,3],[1,6,6,8,3,2,9,3,0]]

token_type_ids:
[[0,0,0,0,0,0,1,1,1],[0,0,0,0,0,1,1,1,0]]

token_type_ids_for_mask:
[[1,1,1,1,1,1,0,0,0],[1,1,1,1,1,0,0,0,-1]]

--------------
labels:
[[5,6,7,8,3,9,6,3],[6,6,8,3,2,9,3,0]]

predictions:
[[5,6,7,8,3,9,6,3],[6,6,8,3,2,9,3,*]]

target_mask:
[[0,0,0,0,0,1,1,1],[0,0,0,0,1,1,1,0]]

attention_mask:
tensor([[[[1., 1., 1., 1., 1., 1., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 1.]]],


        [[[1., 1., 1., 1., 1., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 0., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 0., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 0., 0.],
          [1., 1., 1., 1., 1., 1., 1., 1., 0.],
          [0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])
```